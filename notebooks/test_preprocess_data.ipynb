{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../../../shared_data/xlmr_6L\")\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sample_join_char):\n",
    "    sample = sample_join_char[0]\n",
    "    join_char = sample_join_char[1]\n",
    "    special_char = \"▁\"\n",
    "    len_char_original_token = [len(token) for token in sample[\"words\"]]\n",
    "    new_tokens = tokenizer.tokenize(join_char.join(sample[\"words\"]))\n",
    "    if len(new_tokens) > max_seq_length * 2:\n",
    "        return\n",
    "    new_tags = []\n",
    "    index_original_token = 0\n",
    "    next_tag = False\n",
    "    for token in new_tokens:\n",
    "        _token = token.replace(special_char, \"\")\n",
    "        print([_token, index_original_token, next_tag, len_char_original_token[index_original_token]])\n",
    "        if index_original_token >= len(len_char_original_token):\n",
    "            # self.failed_samples[\"index_oor\"].append(sample)\n",
    "            return\n",
    "        if len(_token) == len_char_original_token[index_original_token]:\n",
    "            if next_tag:\n",
    "                new_tags.append(next_tag)\n",
    "            else:\n",
    "                new_tags.append(sample[\"tags\"][index_original_token])\n",
    "            next_tag = False\n",
    "            index_original_token += 1\n",
    "        elif 0 < len(_token) < len_char_original_token[index_original_token]:\n",
    "            if next_tag:\n",
    "                new_tags.append(next_tag)\n",
    "            else:\n",
    "                new_tags.append(sample[\"tags\"][index_original_token])\n",
    "                next_tag = (\n",
    "                    \"O\"\n",
    "                    if sample[\"tags\"][index_original_token] == \"O\"\n",
    "                    else \"I-\" + sample[\"tags\"][index_original_token][2:]\n",
    "                )\n",
    "            len_char_original_token[index_original_token] -= len(_token)\n",
    "            if len_char_original_token[index_original_token] == 0:\n",
    "                index_original_token += 1\n",
    "                next_tag = False\n",
    "            elif len_char_original_token[index_original_token] < 0:\n",
    "                # self.failed_samples[\"mismatch\"].append(sample)\n",
    "                return\n",
    "        elif len(_token) > len_char_original_token[index_original_token]:\n",
    "            # char_len_new_token = len(_token)\n",
    "            len_merge_token = 0\n",
    "            merge_tags = set()\n",
    "            for j in range(index_original_token, len(sample[\"words\"])):\n",
    "                # print(sample[\"words\"][j], j)\n",
    "                len_merge_token += len_char_original_token[j]\n",
    "                merge_tags.add(sample[\"tags\"][j])\n",
    "                if len_merge_token == len(_token):\n",
    "                    # print([\"merge\", merge_tags, _token])\n",
    "                    if len(merge_tags) == 1:\n",
    "                        new_tags.append(list(merge_tags)[0])\n",
    "                        index_original_token = j + 1\n",
    "                        break\n",
    "                    else:\n",
    "                        return\n",
    "                elif len_merge_token > len(_token):\n",
    "                    print(len_merge_token, _token)\n",
    "                    return\n",
    "\n",
    "        else:\n",
    "            new_tags.append(\"X\")\n",
    "    assert len(new_tokens) == len(new_tags), [\n",
    "        new_tokens,\n",
    "        new_tags,\n",
    "        join_char.join(sample[\"words\"]),\n",
    "    ]\n",
    "    print(new_tokens, len(new_tokens))\n",
    "    print(new_tags, len(new_tags))\n",
    "    # samples.append({\"text\": join_char.join(sample[\"words\"]), \"tags\": new_tags, \n",
    "    #                     \"ori_words\": sample[\"words\"], \"ori_tags\": sample[\"tags\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', 0, False, 3]\n",
      "['미', 0, 'I-PER', 2]\n",
      "['노', 0, 'I-PER', 1]\n",
      "['요', 1, False, 4]\n",
      "['시', 1, 'I-PER', 3]\n",
      "['히', 1, 'I-PER', 2]\n",
      "['코', 1, 'I-PER', 1]\n",
      "['(', 2, False, 1]\n",
      "[',', 3, False, 1]\n",
      "['19', 4, False, 5]\n",
      "['28', 4, 'O', 3]\n",
      "['년', 4, 'O', 1]\n",
      "['1', 5, False, 2]\n",
      "['월', 5, 'O', 1]\n",
      "['22', 6, False, 3]\n",
      "['일', 6, 'O', 1]\n",
      "['~', 7, False, 1]\n",
      "['2004', 8, False, 5]\n",
      "['년', 8, 'O', 1]\n",
      "['2', 9, False, 2]\n",
      "['월', 9, 'O', 1]\n",
      "['27', 10, False, 3]\n",
      "['일', 10, 'O', 1]\n",
      "[')', 11, False, 1]\n",
      "['는', 12, False, 1]\n",
      "['일본', 13, False, 2]\n",
      "['의', 14, False, 1]\n",
      "['역', 15, False, 3]\n",
      "['사가', 15, 'O', 2]\n",
      "['이다', 16, False, 2]\n",
      "['.', 17, False, 1]\n",
      "['▁아', '미', '노', '요', '시', '히', '코', '(', ',', '19', '28', '년', '1', '월', '22', '일', '~', '2004', '년', '2', '월', '27', '일', ')', '는', '일본', '의', '역', '사가', '이다', '.'] 31\n",
      "['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O'] 31\n"
     ]
    }
   ],
   "source": [
    "# sample = {\"words\": ['ジャパン', 'ジャパンカップサイクルロードレース', 'カップ', 'サイクル', 'ロードレース'], \"tags\": ['O', 'O', 'O', 'O', 'O']}\n",
    "# sample = {\"words\": [\"3\", \"月\", \"13\", \"日\"], \"tags\": [\"B-PER\" ,\"O\" ,\"O\" ,\"O\"]}\n",
    "# sample = {\"words\": [\"พร้อม\", \"กัน\", \"นี้\", \"พรรค\", \"มาตุภูมิ\", \"ได้\", \"มีการ\", \"จัด\", \"ปาฐกถา\", \"ใน\", \"หัวข้อ\"], \n",
    "#           \"tags\": [\"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]}\n",
    "# sample = {\"words\": [\"아미노\", \"요시히코\", \"(\", \",\", \"1928년\", \"1월\", \"22일\", \"~\", \"2004년\", \"2월\", \"27일\", \")\", \"는\", \"일본\", \"의\", \"역사가\", \"이다\", \".\"],\n",
    "#           \"tags\": [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\"]}\n",
    "sample = {\n",
    "    \"words\": ['นอกจาก', 'นี้', 'ยัง', 'ขอ', 'ให้', 'รัฐบาล', 'พิจารณา', 'ให้', 'มี', 'รถ', '_', 'ขสมก.', 'ทุก', 'ประเภท', 'ให้', 'บริการ', 'ประชาชน', 'ทุก', 'เส้นทาง', 'บริการ', 'ทั่ว', '_', 'กทม', '.', 'กว่า', '_', '100', '_', 'สาย', '_', 'ให้', 'บริการ', 'ประชาชน', 'อย่าง', 'ทั่วถึง', 'โดย', 'เก็บ', 'ค่า', 'โดยสาร', 'เพียง', 'ครึ่ง', 'ราคา', 'เพื่อ', 'บรรเทา', '_', 'ภาระ', 'ของ', 'ประชาชน', 'ใน', 'ภาวะ', 'เศรษฐกิจ', 'ตกต่ำ'],\n",
    "    \"tags\": ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "}\n",
    "preprocess_data((sample, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '中国人民', '解放', '軍', '海', '軍']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"\".join(sample[\"words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 107814, 69332, 13668, 2750, 13668, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\".join(sample[\"words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af8b634eb6ee2beff9dea1f95fac5ae0278363f238264cab76937a661c169ae1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
